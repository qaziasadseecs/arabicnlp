{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7284452,
          "sourceType": "datasetVersion",
          "datasetId": 4224007
        }
      ],
      "dockerImageVersionId": 30627,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Importing Libraries and Reading Data**"
      ],
      "metadata": {
        "id": "u3fmwdOxuRgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GKfPp9VAuY9Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e242ba2-437f-4f96-8cda-e2551ecad6d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r /content/drive/MyDrive/nlp_arabic/requirements.txt\n",
        "# Import necessary libraries\n",
        "! pip install PyArabic\n",
        "! pip install transformers\n"
      ],
      "metadata": {
        "id": "_xTwnKLFujsG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "102494c2-ad19-475b-e30f-5ef448d9ecae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting absl-py==2.1.0 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 1))\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asttokens==2.4.1 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 2))\n",
            "  Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 3)) (1.6.3)\n",
            "Requirement already satisfied: certifi==2024.2.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 4)) (2024.2.2)\n",
            "Requirement already satisfied: charset-normalizer==3.3.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 5)) (3.3.2)\n",
            "Collecting colorama==0.4.6 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 6))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting comm==0.2.2 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 7))\n",
            "  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: contourpy==1.2.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 8)) (1.2.1)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 9)) (0.12.1)\n",
            "Collecting debugpy==1.8.1 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 10))\n",
            "  Downloading debugpy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting decorator==5.1.1 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 11))\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting executing==2.0.1 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 12))\n",
            "  Downloading executing-2.0.1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: filelock==3.14.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 13)) (3.14.0)\n",
            "Requirement already satisfied: flatbuffers==24.3.25 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 14)) (24.3.25)\n",
            "Requirement already satisfied: fonttools==4.51.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 15)) (4.51.0)\n",
            "Collecting fsspec==2024.3.1 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 16))\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gast==0.5.4 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 17)) (0.5.4)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 18)) (0.2.0)\n",
            "Requirement already satisfied: grpcio==1.63.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 19)) (1.63.0)\n",
            "Collecting h5py==3.11.0 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 20))\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub==0.23.0 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 21))\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna==3.7 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 22)) (3.7)\n",
            "Collecting ipykernel==6.29.4 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 23))\n",
            "  Downloading ipykernel-6.29.4-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipython==8.24.0 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 24))\n",
            "  Downloading ipython-8.24.0-py3-none-any.whl (816 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m816.5/816.5 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jedi==0.19.1 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 25))\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib==1.4.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 26)) (1.4.2)\n",
            "Collecting jupyter_client==8.6.1 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 27))\n",
            "  Downloading jupyter_client-8.6.1-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter_core==5.7.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 28)) (5.7.2)\n",
            "Collecting keras==3.3.3 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 29))\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver==1.4.5 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 30)) (1.4.5)\n",
            "Requirement already satisfied: libclang==18.1.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 31)) (18.1.1)\n",
            "Requirement already satisfied: Markdown==3.6 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 32)) (3.6)\n",
            "Requirement already satisfied: markdown-it-py==3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 33)) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe==2.1.5 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 34)) (2.1.5)\n",
            "Collecting matplotlib==3.8.4 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 35))\n",
            "  Downloading matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib-inline==0.1.7 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 36)) (0.1.7)\n",
            "Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 37)) (0.1.2)\n",
            "Collecting ml-dtypes==0.3.2 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 38))\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting namex==0.0.8 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 39))\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 40)) (1.6.0)\n",
            "Collecting numpy==1.26.4 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 41))\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 42)) (3.3.0)\n",
            "Collecting optree==0.11.0 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 43))\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging==24.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 44)) (24.0)\n",
            "Collecting pandas==2.2.2 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 45))\n",
            "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: parso==0.8.4 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 46)) (0.8.4)\n",
            "Collecting pillow==10.3.0 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 47))\n",
            "  Downloading pillow-10.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs==4.2.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 48)) (4.2.1)\n",
            "Requirement already satisfied: prompt-toolkit==3.0.43 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 49)) (3.0.43)\n",
            "Collecting protobuf==4.25.3 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 50))\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psutil==5.9.8 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 51))\n",
            "  Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pure-eval==0.2.2 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 52))\n",
            "  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
            "Collecting PyArabic==0.6.15 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 53))\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow==16.0.0 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 54))\n",
            "  Downloading pyarrow-16.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pygments==2.18.0 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 55))\n",
            "  Downloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing==3.1.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 56)) (3.1.2)\n",
            "Collecting python-calamine==0.2.0 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 57))\n",
            "  Downloading python_calamine-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dateutil==2.9.0.post0 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 58))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytz==2024.1 (from -r /content/drive/MyDrive/nlp_arabic/requirements.txt (line 59))\n",
            "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement pywin32==306 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pywin32==306\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting PyArabic\n",
            "  Using cached PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from PyArabic) (1.16.0)\n",
            "Installing collected packages: PyArabic\n",
            "Successfully installed PyArabic-0.6.15\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "# !pip install pyarabic\n",
        "# !pip install transformers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras import Input, Model\n",
        "from keras.models import load_model\n",
        "from keras.layers import *\n",
        "from keras import utils\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from transformers import AutoTokenizer, TFAutoModel, AutoConfig\n",
        "import xml.etree.ElementTree as ET\n",
        "from transformers import TFBertModel\n",
        "\n",
        "\n",
        "# Function to read XML data and convert it to a DataFrame\n",
        "def read_xlm_as_dataframe(file):\n",
        "    tree = ET.parse(file)\n",
        "    reviews = tree.getroot()\n",
        "    sent_ids, sentences, targets, polarities = [], [], [], []\n",
        "\n",
        "    for i, sentence in enumerate(reviews.iter('sentence')):\n",
        "        ids, tar, p, texts = [], [], [], []\n",
        "        for text in sentence.iter('text'):\n",
        "            t = text.text\n",
        "        for op in sentence.iter('Opinion'):\n",
        "            tr = op.attrib.get('target')\n",
        "            if tr != \"NULL\":\n",
        "                tar.append(tr)\n",
        "                p.append(op.attrib.get('polarity'))\n",
        "        if len(tar) > 1:\n",
        "            for i in range(len(tar)):\n",
        "                texts.append(t)\n",
        "                ids.append(sentence.attrib.get('id'))\n",
        "        if len(tar) == 1:\n",
        "            texts.append(t)\n",
        "            ids.append(sentence.attrib.get('id'))\n",
        "\n",
        "        sent_ids.extend(ids)\n",
        "        sentences.extend(texts)\n",
        "        targets.extend(tar)\n",
        "        polarities.extend(p)\n",
        "\n",
        "    df = pd.DataFrame(list(zip(sent_ids, sentences, targets, polarities)), columns=['id', 'text', 'term', 'polarity'])\n",
        "    return df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-27T02:26:57.768539Z",
          "iopub.execute_input": "2023-12-27T02:26:57.769175Z",
          "iopub.status.idle": "2023-12-27T02:26:57.783202Z",
          "shell.execute_reply.started": "2023-12-27T02:26:57.769135Z",
          "shell.execute_reply": "2023-12-27T02:26:57.782282Z"
        },
        "trusted": true,
        "id": "gBkQZztduRgY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Loading and Preprocessing Data**"
      ],
      "metadata": {
        "id": "6W6bNlzZuRgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_train = read_xlm_as_dataframe(\"/content/drive/MyDrive/nlp_arabic/AR_Hotels_Train_SB1 (2).xml\")\n",
        "reviews_test = read_xlm_as_dataframe(\"/content/drive/MyDrive/nlp_arabic/AR_HOTE_SB1_TEST (3).xml\")\n",
        "\n",
        "import pyarabic.araby as araby\n",
        "import re\n",
        "\n",
        "\n",
        "def text_preprocessing(text):\n",
        "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = re.sub(\"\\d+\", \" \", text)\n",
        "    text = normalizeArabic(text)\n",
        "    text = re.sub('[A-Za-z]+', ' ', text)\n",
        "    text = re.sub(r'\\\\u[A-Za-z0-9\\\\]+', ' ', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def normalizeArabic(text):\n",
        "    text = text.strip()\n",
        "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "    text = re.sub(noise, '', text)\n",
        "    text = re.sub(r'(.)\\1+', r\"\\1\\1\", text)  # Remove longation\n",
        "    return araby.strip_tashkeel(text)\n",
        "\n",
        "\n",
        "reviews_train['text'] = reviews_train['text'].apply(lambda x: text_preprocessing(x))\n",
        "reviews_train['term'] = reviews_train['term'].apply(lambda x: text_preprocessing(x))\n",
        "reviews_test['text'] = reviews_test['text'].apply(lambda x: text_preprocessing(x))\n",
        "reviews_test['term'] = reviews_test['term'].apply(lambda x: text_preprocessing(x))\n",
        "\n",
        "class_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "reviews_train['label'] = reviews_train['polarity'].map(class_map)\n",
        "reviews_test['label'] = reviews_test['polarity'].map(class_map)\n",
        "\n",
        "\n",
        "def get_one_hot_labels():\n",
        "    enc = OneHotEncoder(handle_unknown='ignore')\n",
        "    train_y = enc.fit_transform(reviews_train['label'].values.reshape(-1, 1)).todense()\n",
        "    test_y = enc.fit_transform(reviews_test['label'].values.reshape(-1, 1)).todense()\n",
        "    return train_y, test_y, enc\n",
        "\n",
        "\n",
        "reviews_train['polarity'].value_counts()\n",
        "\n",
        "reviews_train['label'].value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-27T02:26:57.788164Z",
          "iopub.execute_input": "2023-12-27T02:26:57.788937Z",
          "iopub.status.idle": "2023-12-27T02:27:00.060011Z",
          "shell.execute_reply.started": "2023-12-27T02:26:57.788904Z",
          "shell.execute_reply": "2023-12-27T02:27:00.059041Z"
        },
        "trusted": true,
        "id": "7bExR773uRge",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c5c042f-8ad1-4529-e97e-c5646abb69f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "2    5819\n",
              "0    3141\n",
              "1     660\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3: Tokenization and Model Setup**"
      ],
      "metadata": {
        "id": "SeF5-lhiuRgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_y, test_y, enc = get_one_hot_labels()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('aubmindlab/bert-base-arabertv02')\n",
        "\n",
        "# Tokenize data\n",
        "def tokenize(sentences, aspects, tokenizer):\n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "    for i in range(len(sentences)):\n",
        "        inputs = tokenizer.encode_plus(sentences[i], aspects[i],\n",
        "                                       add_special_tokens=True,\n",
        "                                       max_length=100,\n",
        "                                       pad_to_max_length=True,\n",
        "                                       return_attention_mask=True,\n",
        "                                       return_token_type_ids=True,\n",
        "                                       truncation=True)\n",
        "        input_ids.append(inputs['input_ids'])\n",
        "        input_masks.append(inputs['attention_mask'])\n",
        "\n",
        "\n",
        "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32')\n",
        "\n",
        "\n",
        "X_train = tokenize(reviews_train['text'], reviews_train['term'], tokenizer)\n",
        "X_test = tokenize(reviews_test['text'], reviews_test['term'], tokenizer)\n",
        "\n",
        "\n",
        "# Get the Arabert model and its configuration\n",
        "config = AutoConfig.from_pretrained('aubmindlab/bert-base-arabertv02', output_hidden_states=True, output_attentions=True)\n",
        "arabertembedding = TFAutoModel.from_pretrained('aubmindlab/bert-base-arabertv02', config=config)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-27T02:27:00.062164Z",
          "iopub.execute_input": "2023-12-27T02:27:00.062823Z",
          "iopub.status.idle": "2023-12-27T02:27:09.700597Z",
          "shell.execute_reply.started": "2023-12-27T02:27:00.062786Z",
          "shell.execute_reply": "2023-12-27T02:27:09.699691Z"
        },
        "trusted": true,
        "id": "dph5c-JduRgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "963560fb-b48f-4d5f-96aa-f2dbf82b47b6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'bert.embeddings.position_ids', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 4: Model Architecture and Training**"
      ],
      "metadata": {
        "id": "7bUoBTREuRgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of layers in the Arabert model\n",
        "num_layers = arabertembedding.config.num_hidden_layers\n",
        "\n",
        "\n",
        "# Print the number of layers\n",
        "print(f\"Number of layers in Arabert model: {num_layers}\")\n",
        "\n",
        "\n",
        "#Attention\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, layer_index, name=\"attention\"):\n",
        "        super(Attention, self).__init__()\n",
        "        self.W1 = Dense(units)\n",
        "        self.W2 = Dense(units)\n",
        "        self.V = Dense(1)\n",
        "        self.layer_index = layer_index\n",
        "\n",
        "    def call(self, features):\n",
        "        attention_layer_name = f'attention_{self.layer_index}'  # Update the layer index\n",
        "        hidden = features[:, -1, :]\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n",
        "        score = self.V(attention_hidden_layer)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "\n",
        "# Input layers\n",
        "input_ids_in = Input(shape=(100,), dtype='int32', name=\"input_ids_in\")\n",
        "input_masks_in = Input(shape=(100,), dtype='int32', name=\"input_masks_in\")\n",
        "\n",
        "\n",
        "# Wrap the TensorFlow function within a Keras layer\n",
        "class ArabertEmbeddingLayer(Layer):\n",
        "    def call(self, inputs):\n",
        "        return arabertembedding(input_ids=inputs[0], attention_mask=inputs[1])[2]\n",
        "# Initialize a list to store intermediate outputs from each iteration\n",
        "concatenated_embeddings_list = []\n",
        "# Loop through Arabert layers\n",
        "for layer_index in range(num_layers):\n",
        "    # Extract the hidden states from the specific layer\n",
        "    layer_output = ArabertEmbeddingLayer()([input_ids_in, input_masks_in])[layer_index]\n",
        "\n",
        "    # Apply attention to the Arabert layer output\n",
        "    context_vector, attention_weights = Attention(256, layer_index,name=f'attention_{layer_index}')(layer_output)\n",
        "\n",
        "\n",
        "\n",
        "    # Apply additional layers (e.g., GRU, Dropout)\n",
        "    gru_output = Bidirectional(GRU(256, return_sequences=True))(layer_output)\n",
        "    X2 = tf.keras.layers.Dropout(0.1)(layer_output)\n",
        "    X3 = tf.keras.layers.Dropout(0.1)(layer_output)\n",
        "\n",
        "\n",
        "#   Concatenate the outputs of the layers together\n",
        "    if layer_index == 0:\n",
        "\n",
        "        # If it's the first layer, apply global average pooling along the last axis for tensors with a time dimension\n",
        "        gru_output = GlobalAveragePooling1D()(gru_output)\n",
        "\n",
        "#       context_vector = GlobalAveragePooling1D()(context_vector)\n",
        "        context_vector = context_vector\n",
        "        X2 = GlobalAveragePooling1D()(X2)\n",
        "        X3 = GlobalAveragePooling1D()(X3)\n",
        "\n",
        "\n",
        "        # Now, concatenate along the last axis\n",
        "        all_concatenated_embeddings = tf.concat([gru_output, context_vector, X2, X3], axis=-1)\n",
        "\n",
        "    else:\n",
        "        # If it's not the first layer, make sure the last dimensions match before concatenation\n",
        "        layer_output_shape = layer_output.shape[-1]\n",
        "        gru_output = tf.keras.layers.Dense(layer_output_shape)(GlobalAveragePooling1D()(gru_output))\n",
        "\n",
        "\n",
        "#       context_vector = tf.keras.layers.Dense(layer_output_shape)(GlobalAveragePooling1D()(context_vector))\n",
        "        context_vector = tf.keras.layers.Dense(layer_output_shape)(context_vector)\n",
        "        X2 = tf.keras.layers.Dense(layer_output_shape)(GlobalAveragePooling1D()(X2))\n",
        "        X3 = tf.keras.layers.Dense(layer_output_shape)(GlobalAveragePooling1D()(X3))\n",
        "\n",
        "#       Now, concatenate along the last axis\n",
        "        all_concatenated_embeddings = tf.concat([gru_output, context_vector, X2, X3], axis=-1)\n",
        "\n",
        "    # Append the concatenated embeddings to the list\n",
        "    concatenated_embeddings_list.append(all_concatenated_embeddings)\n",
        "\n",
        "# Concatenate all the intermediate embeddings\n",
        "final_concatenated_embeddings = tf.concat(concatenated_embeddings_list, axis=-1)\n",
        "\n",
        "# Dense layer (x4)\n",
        "X4 = tf.keras.layers.Dense(128, activation='relu')(all_concatenated_embeddings)\n",
        "X5 = tf.keras.layers.Dropout(0.2)(all_concatenated_embeddings)\n",
        "\n",
        "\n",
        "# Output layer (x_last)\n",
        "X_last = tf.keras.layers.Dense(len(enc.categories_[0]), activation='softmax', name=\"output\")(all_concatenated_embeddings)\n",
        "\n",
        "\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=[input_ids_in, input_masks_in], outputs=X_last)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# Model callbacks\n",
        "checkpoint = ModelCheckpoint(\"/content/drive/MyDrive/nlp_arabic/aspect_model_hotel.keras\",\n",
        "                             monitor='val_accuracy',\n",
        "                             mode='max',\n",
        "                             save_best_only=True,\n",
        "                             save_weights_only=False,\n",
        "                             verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Compile Model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=2e-5), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# Model training\n",
        "# # Example: Early stopping\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "history = model.fit(X_train, train_y, validation_split=0.05, epochs=10, batch_size=16, verbose=1)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-27T02:27:09.702179Z",
          "iopub.execute_input": "2023-12-27T02:27:09.702533Z",
          "iopub.status.idle": "2023-12-27T03:07:21.339823Z",
          "shell.execute_reply.started": "2023-12-27T02:27:09.702504Z",
          "shell.execute_reply": "2023-12-27T03:07:21.338906Z"
        },
        "trusted": true,
        "id": "xEs9L-ZRuRgi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3c281544-03b5-405d-ed58-097368a39368"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of layers in Arabert model: 12\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Exception encountered when calling layer 'embeddings' (type TFBertEmbeddings).\n\nCould not build a TypeSpec for name: \"tf.debugging.assert_less_5/assert_less/Assert/Assert\"\nop: \"Assert\"\ninput: \"tf.debugging.assert_less_5/assert_less/All\"\ninput: \"tf.debugging.assert_less_5/assert_less/Assert/Assert/data_0\"\ninput: \"tf.debugging.assert_less_5/assert_less/Assert/Assert/data_1\"\ninput: \"tf.debugging.assert_less_5/assert_less/Assert/Assert/data_2\"\ninput: \"Placeholder\"\ninput: \"tf.debugging.assert_less_5/assert_less/Assert/Assert/data_4\"\ninput: \"tf.debugging.assert_less_5/assert_less/y\"\nattr {\n  key: \"T\"\n  value {\n    list {\n      type: DT_STRING\n      type: DT_STRING\n      type: DT_STRING\n      type: DT_INT32\n      type: DT_STRING\n      type: DT_INT32\n    }\n  }\n}\nattr {\n  key: \"summarize\"\n  value {\n    i: 3\n  }\n}\n of unsupported type <class 'tensorflow.python.framework.ops.Operation'>.\n\nCall arguments received by layer 'embeddings' (type TFBertEmbeddings):\n  • input_ids=<KerasTensor: shape=(None, 100) dtype=int32 (created by layer 'input_ids_in')>\n  • position_ids=None\n  • token_type_ids=<KerasTensor: shape=(None, 100) dtype=int32 (created by layer 'tf.fill_5')>\n  • inputs_embeds=None\n  • past_key_values_length=0\n  • training=False",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-3b80ed1106dd>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Get Arabert embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0marabert_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marabertembedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_masks_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Get the last layer of hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mrun_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0munpacked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munpacked_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Keras enforces the first layer argument to be passed, and checks it through `inspect.getfullargspec()`. This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m   1211\u001b[0m             `past_key_values`). Set to `False` during training, `True` during generation\n\u001b[1;32m   1212\u001b[0m         \"\"\"\n\u001b[0;32m-> 1213\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1214\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mrun_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0munpacked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munpacked_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Keras enforces the first layer argument to be passed, and checks it through `inspect.getfullargspec()`. This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    892\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, position_ids, token_type_ids, inputs_embeds, past_key_values_length, training)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mcheck_embeddings_within_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tf_utils.py\u001b[0m in \u001b[0;36mcheck_embeddings_within_bounds\u001b[0;34m(tensor, embed_dim, tensor_name)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mtensor_name\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moptional\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0merror\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \"\"\"\n\u001b[0;32m--> 163\u001b[0;31m     tf.debugging.assert_less(\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/tf_op_layer.py\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self, op, args, kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         ):\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mTFOpLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling layer 'embeddings' (type TFBertEmbeddings).\n\nCould not build a TypeSpec for name: \"tf.debugging.assert_less_5/assert_less/Assert/Assert\"\nop: \"Assert\"\ninput: \"tf.debugging.assert_less_5/assert_less/All\"\ninput: \"tf.debugging.assert_less_5/assert_less/Assert/Assert/data_0\"\ninput: \"tf.debugging.assert_less_5/assert_less/Assert/Assert/data_1\"\ninput: \"tf.debugging.assert_less_5/assert_less/Assert/Assert/data_2\"\ninput: \"Placeholder\"\ninput: \"tf.debugging.assert_less_5/assert_less/Assert/Assert/data_4\"\ninput: \"tf.debugging.assert_less_5/assert_less/y\"\nattr {\n  key: \"T\"\n  value {\n    list {\n      type: DT_STRING\n      type: DT_STRING\n      type: DT_STRING\n      type: DT_INT32\n      type: DT_STRING\n      type: DT_INT32\n    }\n  }\n}\nattr {\n  key: \"summarize\"\n  value {\n    i: 3\n  }\n}\n of unsupported type <class 'tensorflow.python.framework.ops.Operation'>.\n\nCall arguments received by layer 'embeddings' (type TFBertEmbeddings):\n  • input_ids=<KerasTensor: shape=(None, 100) dtype=int32 (created by layer 'input_ids_in')>\n  • position_ids=None\n  • token_type_ids=<KerasTensor: shape=(None, 100) dtype=int32 (created by layer 'tf.fill_5')>\n  • inputs_embeds=None\n  • past_key_values_length=0\n  • training=False"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 5: Model Evaluation and Visualization**"
      ],
      "metadata": {
        "id": "nYamf1P8uRgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "\n",
        "\n",
        "# Define custom_objects dictionary\n",
        "custom_objects = {\n",
        "    \"Attention\": Attention,\n",
        "    \"TFAutoModel\": TFAutoModel,\n",
        "    \"TFBertModel\": TFBertModel,\n",
        "    \"arabertembedding\": arabertembedding\n",
        "}\n",
        "\n",
        "# Load the best model\n",
        "best_model = tf.keras.models.load_model('/content/drive/MyDrive/nlp_arabic/aspect_model_hotel.keras', custom_objects=custom_objects)\n",
        "\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_model.evaluate(X_test, test_y)\n",
        "\n",
        "\n",
        "# Function to convert RGB to hex\n",
        "def rgb_to_hex(rgb):\n",
        "    return '#%02x%02x%02x' % rgb\n",
        "\n",
        "def attention2color(attention_score, label):\n",
        "    r = 255 - int(attention_score * 255)\n",
        "    color = rgb_to_hex((255, r, r))\n",
        "    return str(color)\n",
        "\n",
        "def visualize_attention():\n",
        "    # Dynamic search for attention layers\n",
        "    attention_layers = [layer for layer in best_model.layers if isinstance(layer, Attention)]\n",
        "\n",
        "    # Ensure there are attention layers\n",
        "    if not attention_layers:\n",
        "        print(\"No attention layers found.\")\n",
        "        return\n",
        "\n",
        "    for layer_index, attention_layer in enumerate(attention_layers):\n",
        "        attention_layer_name = attention_layer.name\n",
        "\n",
        "        model_att = Model(inputs=best_model.input, outputs=[best_model.output, attention_layer.output[-1]])\n",
        "\n",
        "        idx = np.random.randint(low=0, high=X_test[0].shape[0])  # Get a random test\n",
        "        tokenized_sample = np.trim_zeros(X_test[0][idx])  # Get the tokenized text\n",
        "        label_probs, attentions = model_att.predict((X_test[0][idx].reshape(1, -1), X_test[1][idx].reshape(1, -1)))\n",
        "\n",
        "\n",
        "\n",
        "    # Perform the prediction\n",
        "    po=reviews_test['polarity'][idx]\n",
        "    aspect=reviews_test['term'][idx]\n",
        "    # print(idx)\n",
        "    # Get decoded text and labels\n",
        "    decoded_text = tokenizer.decode(tokenized_sample)\n",
        "    #print(decoded_text)\n",
        "\n",
        "\n",
        "    # Get classification\n",
        "    label = np.argmax(label_probs) # Only one\n",
        "    # print(label)\n",
        "    #label2id = ['negative', 'neutral', 'positive']\n",
        "\n",
        "\n",
        "    # Get word attentions using attenion vector\n",
        "    token_attention_dic = {}\n",
        "    max_score = 0.0\n",
        "    min_score = 0.0\n",
        "\n",
        "    attentions_text = attentions[-len(tokenized_sample):]\n",
        "    attentions_text = (attentions_text - np.min(attentions_text)) / (np.max(attentions_text) - np.min(attentions_text))\n",
        "    for token, attention_score in zip(decoded_text.split(), attentions_text):\n",
        "        #print(token, attention_score)\n",
        "        if token not in [\"[SEP]\", \"[CLS]\"]:\n",
        "          token_attention_dic[token] = attention_score\n",
        "\n",
        "\n",
        "    # Build HTML String to viualize attentions\n",
        "    html_text = \"<hr><p style='font-size: large'><b>Text:  </b>\"\n",
        "    html_text1 = \"<hr><p style='font-size: large'><b>Aspect:  </b>\"+aspect\n",
        "    html_text2 = \"<hr><p style='font-size: large'><b>Polarity:  </b>\"+po\n",
        "    for token, attention in token_attention_dic.items():\n",
        "        html_text += \"<span style='background-color:{};'>{} <span> \".format(attention2color(attention, label), token)\n",
        "\n",
        "\n",
        "\n",
        "    # Display text enriched with attention scores\n",
        "    display(HTML(html_text))\n",
        "    display(HTML(html_text1))\n",
        "    display(HTML(html_text2))\n",
        "    # PLOT EMOTION SCORES\n",
        "    _labels = ['negative', 'neutral', 'positive']\n",
        "\n",
        "    probs = label_probs\n",
        "    plt.figure(figsize=(5,1.5))\n",
        "    plt.bar(np.arange(len(_labels)), probs.squeeze(), align='center', alpha=0.7, color=[ 'red', 'yellow', 'green'])\n",
        "    plt.xticks(np.arange(len(_labels)), _labels)\n",
        "    plt.ylabel('Scores')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Apply the visualize_attention function to see the attention visualization\n",
        "for layer_index in range(num_layers):\n",
        "#     visualize_attention()\n",
        "    visualize_attention()\n",
        "\n",
        "\n",
        "def tokenize_one_sen(sentence,aspect, tokenizer):\n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "\n",
        "    inputs = tokenizer.encode_plus(sentence, aspect,\n",
        "                                       add_special_tokens=True,\n",
        "                                       max_length=100,\n",
        "                                       pad_to_max_length=True,\n",
        "                                       return_attention_mask=True,\n",
        "                                       return_token_type_ids=True, truncation=True)\n",
        "    input_ids.append(inputs['input_ids'])\n",
        "    input_masks.append(inputs['attention_mask'])\n",
        "    input_segments.append(inputs['token_type_ids'])\n",
        "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32')\n",
        "\n",
        "\n",
        "# Tokenize and predict for a single sentence and aspect\n",
        "text = 'وكذلك اود ان اشير الي حسن الضيافه والاستقبال من طاقم الاستقبال و اخص المستر اسامه خليفه لتفانيه في خدمه العملاء'\n",
        "aspect = 'حسن الضيافه'\n",
        "\n",
        "#preprocessing\n",
        "text = text_preprocessing(text)\n",
        "aspect = text_preprocessing(aspect)\n",
        "\n",
        "check_sen = tokenize_one_sen(text, aspect, tokenizer)\n",
        "\n",
        "\n",
        "output = best_model.predict(check_sen)\n",
        "print(output)\n",
        "label = output.argmax()\n",
        "sentiment = enc.categories_[0][label]\n",
        "print(sentiment)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-27T03:07:21.342748Z",
          "iopub.execute_input": "2023-12-27T03:07:21.343340Z",
          "iopub.status.idle": "2023-12-27T03:08:31.399162Z",
          "shell.execute_reply.started": "2023-12-27T03:07:21.343300Z",
          "shell.execute_reply": "2023-12-27T03:08:31.398032Z"
        },
        "trusted": true,
        "id": "7Xp28oP5uRgj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7c2b684-8dcd-45a9-9e7d-20f65109ee25"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75/75 [==============================] - 19s 248ms/step - loss: 0.3833 - accuracy: 0.8651\n",
            "No attention layers found.\n",
            "No attention layers found.\n",
            "No attention layers found.\n",
            "No attention layers found.\n",
            "No attention layers found.\n",
            "No attention layers found.\n",
            "No attention layers found.\n",
            "No attention layers found.\n",
            "No attention layers found.\n",
            "No attention layers found.\n",
            "No attention layers found.\n",
            "No attention layers found.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 10s 10s/step\n",
            "[[0.00543682 0.00975551 0.9848076 ]]\n",
            "2\n"
          ]
        }
      ]
    }
  ]
}